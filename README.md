# Ironia-MVP: Detec√ß√£o de Ironia em Redes Sociais (PT-BR)

Este projeto implementa um pipeline de detec√ß√£o de ironia em textos de redes sociais em portugu√™s brasileiro, utilizando modelos de aprendizado de m√°quina cl√°ssicos (SVM) e modelos transformadores avan√ßados (BERTimbau).

---

## Estrutura de Entrada

O arquivo CSV deve conter ao menos:

| Coluna | Descri√ß√£o |
|--------|-----------|
| `text` | Conte√∫do textual do post |
| `label` | 1 = ir√¥nico, 0 = n√£o-ir√¥nico |

Opcionalmente:

| Coluna | Papel |
|--------|------|
| `author_id` | Controlar vazamento de usu√°rio entre train/val/test |
| `timestamp` | Permitir split temporal sem mistura de √©pocas |

---
## Depend√™ncias

numpy	- array e c√°lculos num√©ricos
pandas	- leitura do CSV e manipula√ß√£o de dados
scikit-learn	- SVM, TF-IDF, m√©tricas, splits
scipy	- hstack de matrizes esparsas
ftfy	- corre√ß√£o de encoding
emoji	- detec√ß√£o/substitui√ß√£o de emojis
unidecode	- suporte opcional ao texto
transformers	- modelo BERTimbau e Trainer
datasets	- DatasetDict do HuggingFace
evaluate	- m√©tricas HF (f1, accuracy)
accelerate	- otimiza√ß√µes de treino
torch	- rede neural do BERT

## Como Executar

Crie e ative um ambiente virtual (opcional, mas recomendado):

```bash
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\Activate.ps1
````

Para instalar as depend√™ncias
````bash
pip install numpy pandas scikit-learn scipy ftfy emoji unidecode
pip install transformers datasets evaluate accelerate
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
````
Para rodar o c√≥digo, √© necess√°rio dar uma base de dados como input. J√° temos uma de exemplo aqui neste reposit√≥rio

Para rodar somente o SVM
````bash
python ironia_mvp.py --data tweets.csv --do_svm
````

Para rodar somente o modelo BERTimbau
````bash
python ironia_mvp.py --data tweets.csv --do_bert
````

Para rodar ambos os modelos
````bash
python ironia_mvp.py --data tweets.csv --do_svm --do_bert
````

##Par√¢metros Opcionais

--no_group_user	- Ignora agrupamento por autor mesmo se houver author_id
--group_time	- Faz split temporal se houver timestamp
--epochs	- N√∫mero de √©pocas para o BERT
--max_len	- Tamanho m√°ximo dos tokens
--lr	- Taxa de aprendizado
--batch	- Tamanho do batch


##üìä Sa√≠das e M√©tricas

Ao final da execu√ß√£o s√£o exibidos:
üìà F1-macro por classe
üìâ Matriz de confus√£o
üìå Relat√≥rio completo (precision / recall / F1)
üì¶ Melhor modelo BERT √© salvo em out_bert/



##Licenciamento

‚úÖ Licen√ßa MIT 
